{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOTOuEoa7F9q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import platform\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-7XIl2T6gqy"
   },
   "source": [
    "## CS-Ratio and Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\python_dir\\\\knee_csmri'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbrZuVDQ7JOu"
   },
   "outputs": [],
   "source": [
    "cs_ratio = 20\n",
    "end_epoch = 13\n",
    "start_epoch = 0\n",
    "learning_rate = 1e-4\n",
    "#Sampling matrix\n",
    "data_dir='train_data'\n",
    "mask_dir='mask_dir'\n",
    "mask_type='q1'    #q1 for DLMRi masks\n",
    "##############\n",
    "layer_num =9\n",
    "group_num = 1\n",
    "#############\n",
    "nrtrain = 600   # number of training blocks\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXtJ5U5IGHB1"
   },
   "outputs": [],
   "source": [
    "gpu_list = '0'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_list\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load CS Sampling Matrix: phi\n",
    "Phi_data_Name = './%s/rand_%s_%d.mat'  %(mask_dir,mask_type,cs_ratio)\n",
    "Phi_data = sio.loadmat(Phi_data_Name)\n",
    "mask_matrix = Phi_data['mask_matrix']\n",
    "mask_matrix = torch.from_numpy(mask_matrix).type(torch.FloatTensor)\n",
    "mask = torch.unsqueeze(mask_matrix, 2)\n",
    "mask = torch.cat([mask, mask], 2)\n",
    "mask = mask.to(device)\n",
    "#Load train data\n",
    "Training_data_Name = './%s/kneemrtrainingnorm256.mat'%(data_dir)\n",
    "Training_data = sio.loadmat(Training_data_Name)\n",
    "Training_labels = Training_data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xJ-78_CGTsl"
   },
   "source": [
    "Define ista-Net NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nh7u9XmBGrNM"
   },
   "outputs": [],
   "source": [
    "class FFT_Mask_ForBack(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFT_Mask_ForBack, self).__init__()\n",
    "    def forward(self, x, mask):\n",
    "        x_dim_0 = x.shape[0]\n",
    "        x_dim_1 = x.shape[1]\n",
    "        x_dim_2 = x.shape[2]\n",
    "        x_dim_3 = x.shape[3]\n",
    "    \n",
    "        x = x.view(-1, x_dim_2, x_dim_3, 1)\n",
    "        \n",
    "        y = torch.zeros_like(x)\n",
    "       \n",
    "        z = torch.cat([x, y], 3)\n",
    "        \n",
    "        fftz = torch.fft(z, 2)\n",
    "        \n",
    "        z_hat = torch.ifft(fftz * mask, 2)\n",
    "        \n",
    "        x = z_hat[:, :, :, 0:1]\n",
    "       \n",
    "        x = x.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)\n",
    "      \n",
    "        return x\n",
    "# Define ISTA-Net-plus Block\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.lambda_step = nn.Parameter(torch.Tensor([0.5]))\n",
    "        self.soft_thr = nn.Parameter(torch.Tensor([0.01]))\n",
    "        self.W1 = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 1, 3, 3)))\n",
    "        self.conv1_forward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))\n",
    "        self.conv2_forward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))\n",
    "        self.conv1_backward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))\n",
    "        self.conv2_backward = nn.Parameter(init.xavier_normal_(torch.Tensor(32, 32, 3, 3)))\n",
    "        self.conv_G = nn.Parameter(init.xavier_normal_(torch.Tensor(1, 32, 3, 3)))\n",
    "        self.c1 = nn.Parameter(torch.ones(1,1,1,1), requires_grad=True)\n",
    "        self.c2 = nn.Parameter(torch.ones(1,1,1,1), requires_grad=True)\n",
    "        self.c3 = nn.Parameter(torch.ones(1,1,1,1), requires_grad=True)        \n",
    "        self.b1 = nn.Parameter(torch.zeros(1,32,1,1), requires_grad=True)\n",
    "        self.b2 = nn.Parameter(torch.zeros(1,32,1,1), requires_grad=True)\n",
    "        self.b3 = nn.Parameter(torch.zeros(1,32,1,1), requires_grad=True)\n",
    "    def forward(self, x, fft_forback, PhiTb, mask):\n",
    "        x = x - self.lambda_step * fft_forback(x, mask)\n",
    "        x = x + self.lambda_step * PhiTb\n",
    "        x_input = x\n",
    "        gamma1 = F.conv2d(x_input, self.W1, padding=1)\n",
    "        gamma2 = self.c1*F.conv2d(gamma1, self.conv1_forward, padding=1)+self.b1\n",
    "        gamma2 = F.relu(gamma2)\n",
    "        gamma3 = F.conv2d(gamma2, self.conv2_forward, padding=1)\n",
    "        for _ in  range(1):            \n",
    "            # backward computation\n",
    "            gamma2 = F.conv_transpose2d(gamma3,self.conv2_forward,padding = 1)\n",
    "            gamma1 = F.conv_transpose2d(gamma2,self.conv1_forward,padding = 1)            \n",
    "            # forward computation\n",
    "            gamma1 = F.relu( (gamma1 - self.c1 * F.conv2d( F.conv_transpose2d(gamma1,self.W1,padding=1) - x ,self.W1,padding=1)) + self.b1)\n",
    "            gamma2 = F.relu( (gamma2 - self.c2 * F.conv2d( F.conv_transpose2d(gamma2,self.conv1_forward,padding=1) - gamma1, self.conv1_forward,padding=1)) + self.b2) \n",
    "            gamma3 = F.relu( (gamma3 - self.c3 * F.conv2d( F.conv_transpose2d(gamma3,self.conv2_forward,padding=1) - gamma2, self.conv2_forward,padding=1)) + self.b3) \n",
    "        gammaE = torch.mul(torch.sign(gamma3), F.relu(torch.abs(gamma3) - self.soft_thr))\n",
    "        gamma4 = F.conv2d(gammaE, self.conv1_backward, padding=1)\n",
    "        gamma4 = F.relu(gamma4)\n",
    "        gamma5 = F.conv2d(gamma4, self.conv2_backward, padding=1)\n",
    "        gamma6 = F.conv2d(gamma5, self.conv_G, padding=1)\n",
    "        x_pred = x_input + gamma6\n",
    "        x = F.conv2d(gamma3, self.conv1_backward, padding=1)\n",
    "        x = F.relu(x)\n",
    "        x_D_est = F.conv2d(x, self.conv2_backward, padding=1)\n",
    "        symloss = x_D_est - gamma1\n",
    "        return [x_pred, symloss]\n",
    "# Define ISTA-Net-plus\n",
    "class ISTANetplus(torch.nn.Module):\n",
    "    def __init__(self, LayerNo):\n",
    "        super(ISTANetplus, self).__init__()\n",
    "        onelayer = []\n",
    "        self.LayerNo = LayerNo\n",
    "        self.fft_forback = FFT_Mask_ForBack()\n",
    "        for i in range(LayerNo):\n",
    "            onelayer.append(BasicBlock())\n",
    "        self.fcs = nn.ModuleList(onelayer)\n",
    "    def forward(self, PhiTb, mask):\n",
    "        x = PhiTb\n",
    "        layers_sym = []   # for computing symmetric loss\n",
    "        for i in range(self.LayerNo):\n",
    "            [x, layer_sym] = self.fcs[i](x, self.fft_forback, PhiTb, mask)\n",
    "            layers_sym.append(layer_sym)\n",
    "        x_final = x\n",
    "        return [x_final, layers_sym]\n",
    "model = ISTANetplus(layer_num)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "print_flag = 1   # print parameter number\n",
    "if print_flag:\n",
    "    num_count = 0\n",
    "    for para in model.parameters():\n",
    "        num_count += 1\n",
    "        #print('Layer %d' % num_count)\n",
    "        #print(para.size())\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, data, length):\n",
    "        self.data = data\n",
    "        self.len = length\n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.data[index, :]).float()\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "##\n",
    "model_dir='model_dir_knee'\n",
    "log_dir='log_dir_knee'\n",
    "##\n",
    "rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=0,shuffle=True)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model_dir = \"./%s/MRI_CS_ISTA_Net_plus_layer_%d_group_%d_ratio_%d\" % (model_dir, layer_num, group_num, cs_ratio)\n",
    "log_file_name = \"./%s/Log_MRI_CS_ISTA_Net_plus_layer_%d_group_%d_ratio_%d.txt\" % (log_dir, layer_num, group_num, cs_ratio)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4UWruI-ZHXd"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQEFtw2ZHFmR"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 608699,
     "status": "ok",
     "timestamp": 1595648482920,
     "user": {
      "displayName": "W T",
      "photoUrl": "",
      "userId": "18193602041679892038"
     },
     "user_tz": -300
    },
    "id": "qDE1GB1EScl4",
    "outputId": "ee8bcc31-06f9-4325-b0c7-69b2267e7ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/13] Total Loss: 0.00081, Discrepancy Loss: 0.00073,  Constraint Loss: 0.00863\n",
      "\n",
      "[01/13] Total Loss: 0.00042, Discrepancy Loss: 0.00037,  Constraint Loss: 0.00485\n",
      "\n",
      "[02/13] Total Loss: 0.00053, Discrepancy Loss: 0.00049,  Constraint Loss: 0.00328\n",
      "\n",
      "[03/13] Total Loss: 0.00045, Discrepancy Loss: 0.00042,  Constraint Loss: 0.00272\n",
      "\n",
      "[04/13] Total Loss: 0.00041, Discrepancy Loss: 0.00039,  Constraint Loss: 0.00233\n",
      "\n",
      "[05/13] Total Loss: 0.00031, Discrepancy Loss: 0.00029,  Constraint Loss: 0.00182\n",
      "\n",
      "[06/13] Total Loss: 0.00025, Discrepancy Loss: 0.00024,  Constraint Loss: 0.00178\n",
      "\n",
      "[07/13] Total Loss: 0.00066, Discrepancy Loss: 0.00064,  Constraint Loss: 0.00161\n",
      "\n",
      "[08/13] Total Loss: 0.00037, Discrepancy Loss: 0.00035,  Constraint Loss: 0.00164\n",
      "\n",
      "[09/13] Total Loss: 0.00029, Discrepancy Loss: 0.00028,  Constraint Loss: 0.00138\n",
      "\n",
      "[10/13] Total Loss: 0.00037, Discrepancy Loss: 0.00035,  Constraint Loss: 0.00172\n",
      "\n",
      "[11/13] Total Loss: 0.00028, Discrepancy Loss: 0.00026,  Constraint Loss: 0.00136\n",
      "\n",
      "[12/13] Total Loss: 0.00056, Discrepancy Loss: 0.00054,  Constraint Loss: 0.00140\n",
      "\n",
      "Duration: 0:14:44.111765\n"
     ]
    }
   ],
   "source": [
    "if start_epoch > 0:\n",
    "    pre_model_dir = model_dir\n",
    "    model.load_state_dict(torch.load('./%s/net_params_%d.pkl' % (pre_model_dir, start_epoch)))\n",
    "#save metrics\n",
    "total_loss = np.zeros((end_epoch,))\n",
    "loss_disc = np.zeros((end_epoch,))\n",
    "loss_const = np.zeros((end_epoch,))\n",
    "# Training loop\n",
    "start_time = datetime.now()\n",
    "for epoch_i in range(start_epoch, end_epoch):\n",
    "    for data in rand_loader:\n",
    "        batch_x = data\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_x = batch_x.view(batch_x.shape[0], 1, batch_x.shape[1], batch_x.shape[2])\n",
    "        start = time() \n",
    "        PhiTb = FFT_Mask_ForBack()(batch_x, mask)\n",
    "        [x_output, loss_layers_sym] = model(PhiTb, mask)\n",
    "        end = time()\n",
    "        # Compute and print loss\n",
    "        loss_discrepancy = torch.mean(torch.pow(x_output - batch_x, 2))\n",
    "        loss_constraint = torch.mean(torch.pow(loss_layers_sym[0], 2))\n",
    "        for k in range(layer_num-1):\n",
    "            loss_constraint += torch.mean(torch.pow(loss_layers_sym[k+1], 2))\n",
    "        gamma = torch.Tensor([0.01]).to(device)\n",
    "        # loss_all = loss_discrepancy\n",
    "        loss_all = loss_discrepancy + torch.mul(gamma, loss_constraint)\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss_all.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    output_data = \"[%02d/%02d] Total Loss: %.5f, Discrepancy Loss: %.5f,  Constraint Loss: %.5f\\n\" % (epoch_i, end_epoch, loss_all.item(), loss_discrepancy.item(), loss_constraint)\n",
    "    print(output_data)\n",
    "   # Save loss in text files for plots\n",
    "    output_data1 = \"%.6f,\" % (loss_all.item())\n",
    "    output_data2 = \"%.6f,\" % (loss_discrepancy.item() )\n",
    "    output_data3 = \"%.6f,\" % (loss_constraint)\n",
    "    output_file_name1 = \"./%s/total_loss__%d.txt\" % (log_dir, cs_ratio)\n",
    "    output_file_name2 = \"./%s/disc_loss_%d.txt\" % (log_dir, cs_ratio)\n",
    "    output_file_name3 = \"./%s/const_loss_%d.txt\" % (log_dir, cs_ratio)\n",
    "    output_file1 = open(output_file_name1, 'a')\n",
    "    output_file1.write(output_data1)\n",
    "    output_file1.close() \n",
    "    output_file2 = open(output_file_name2, 'a')\n",
    "    output_file2.write(output_data2)\n",
    "    output_file2.close()\n",
    "    output_file3 = open(output_file_name3, 'a')  \n",
    "    output_file3.write(output_data3)   \n",
    "    output_file3.close()\n",
    "    #    \n",
    "    total_loss[epoch_i] = loss_all.item()\n",
    "    loss_disc[epoch_i]  = loss_discrepancy.item() \n",
    "    loss_const[epoch_i] = loss_constraint \n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8Rmpe4i23_k"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./%s/net_params_%d.pkl\" %(model_dir,end_epoch))  # save only the parameters"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2.1-KNEE_train_22_june.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
